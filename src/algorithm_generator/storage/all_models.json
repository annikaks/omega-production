{
  "models": [
    {
      "id": "91a9766e-21fb-4e41-8b6f-356140ff10f6",
      "name": "SimpleRandomForestClassifier",
      "filename": "simple_random_forest_classifier.py",
      "description": "can you make a simple random forest classifier\n",
      "metrics": {
        "Iris": 0.9,
        "Wine": 1.0,
        "Breast Cancer": 0.956140350877193,
        "Digits": 0.9666666666666667,
        "Balance Scale": 0.85,
        "Blood Transfusion": 0.7533333333333333,
        "Haberman": 0.6451612903225806,
        "Seeds": 0.9047619047619048,
        "Teaching Assistant": 0.6129032258064516,
        "Zoo": 0.9047619047619048,
        "Planning Relax": 0.7567567567567568,
        "Ionosphere": 0.9577464788732394,
        "Sonar": 0.8571428571428571,
        "Glass": 0.8604651162790697,
        "Vehicle": 0.7470588235294118,
        "Liver Disorders": 0.9178082191780822,
        "Heart Statlog": 0.8333333333333334,
        "Pima Indians Diabetes": 0.7402597402597403,
        "Australian": 0.950381679389313,
        "Monks-1": 1.0
      },
      "timestamp": 1766999123.201417,
      "total_score": 0.8708585164835163,
      "summary": "An ensemble learning method that constructs multiple decision trees during training, where each tree is trained on a bootstrap sample of the data and considers only a random subset of features at each split (feature bagging). Predictions are made by aggregating votes from all trees through majority voting for classification, which reduces overfitting and variance compared to individual decision trees by leveraging the wisdom of crowds principle where uncorrelated weak learners combine to form a strong learner."
    },
    {
      "id": "19f81488-9853-4004-b411-b3f9dbc3e78c",
      "name": "NeuroSymbolicRBFClassifier",
      "filename": "neuro_symbolic_rbf_classifier.py",
      "description": "Synthesize a neuro-symbolic classifier that utilizes a shallow Radial Basis Function (RBF) network to extract non-linear kernels, which are then fed into a symbolic rule-induction engine. The model must dynamically weight the influence of the symbolic rules based on the local density of the training data in the feature space, using a custom implementation of a Gaussian Mixture Model to determine that density",
      "metrics": {
        "Iris": 0.7666666666666667,
        "Wine": 1.0,
        "Breast Cancer": 0.8245614035087719,
        "Digits": 0.5777777777777777,
        "Balance Scale": 0.85,
        "Blood Transfusion": 0.6266666666666667,
        "Haberman": 0.6935483870967742,
        "Seeds": 0.9047619047619048,
        "Teaching Assistant": 0.5806451612903226,
        "Zoo": 0.8571428571428571,
        "Planning Relax": 0.5675675675675675,
        "Ionosphere": 0.9154929577464789,
        "Sonar": 0.5476190476190477,
        "Glass": 0.5116279069767442,
        "Vehicle": 0.48823529411764705,
        "Liver Disorders": 0.8664383561643836,
        "Heart Statlog": 0.7962962962962963,
        "Pima Indians Diabetes": 0.6233766233766234,
        "Australian": 0.7366412213740458,
        "Monks-1": 0.6607142857142857
      },
      "timestamp": 1767001273.69254,
      "total_score": 0.38159448498450577,
      "summary": "A hybrid classifier that transforms input data into a radial basis function (RBF) feature space using Gaussian kernels centered at K-means cluster centers, then fits a Gaussian Mixture Model for density estimation in this transformed space. It extracts symbolic if-then rules by identifying discriminative RBF activation patterns per class (using threshold-based conditions with confidence and support metrics), and makes predictions by combining rule-based voting weighted by both rule confidence and local GMM density, falling back to nearest-neighbor search when no rules activate."
    },
    {
      "id": "599b6a49-22d9-4fa3-ae16-e936dc8bb28f",
      "name": "SimpleDecisionTreeClassifier",
      "filename": "simple_decision_tree_classifier.py",
      "description": "make a simple decision tree\n",
      "metrics": {
        "Iris": 0.9333333333333333,
        "Wine": 0.9166666666666666,
        "Breast Cancer": 0.9298245614035088,
        "Digits": 0.6638888888888889,
        "Balance Scale": 0.95,
        "Blood Transfusion": 0.8,
        "Haberman": 0.6451612903225806,
        "Seeds": 0.9047619047619048,
        "Teaching Assistant": 0.6451612903225806,
        "Zoo": 0.8571428571428571,
        "Planning Relax": 0.6216216216216216,
        "Ionosphere": 0.9014084507042254,
        "Sonar": 0.7142857142857143,
        "Glass": 0.6976744186046512,
        "Vehicle": 0.6647058823529411,
        "Liver Disorders": 0.3913894324853229,
        "Heart Statlog": 0.8148148148148148,
        "Pima Indians Diabetes": 0.7987012987012987,
        "Australian": 0.9351145038167938,
        "Monks-1": 0.7857142857142857
      },
      "timestamp": 1767004874.63345,
      "total_score": 0.6151036915648659
    },
    {
      "id": "ac590f20-eea0-411c-bb4c-adcc8521d7ca",
      "name": "GiniDecisionTreeClassifier",
      "filename": "gini_decision_tree_classifier.py",
      "description": "Develop a simple Decision Tree classifier with a maximum depth of 3. The tree should split nodes based on the Gini Impurity metric. Ensure the code is self-contained and does not rely on external tree-building libraries",
      "metrics": {
        "Iris": 0.9666666666666667,
        "Wine": 0.9722222222222222,
        "Breast Cancer": 0.9385964912280702,
        "Digits": 0.48333333333333334,
        "Balance Scale": 0.8,
        "Blood Transfusion": 0.8,
        "Haberman": 0.6451612903225806,
        "Seeds": 0.8809523809523809,
        "Teaching Assistant": 0.5161290322580645,
        "Zoo": 0.8095238095238095,
        "Planning Relax": 0.7027027027027027,
        "Ionosphere": 0.9295774647887324,
        "Sonar": 0.7142857142857143,
        "Glass": 0.6976744186046512,
        "Vehicle": 0.6882352941176471,
        "Liver Disorders": 0.2788649706457926,
        "Heart Statlog": 0.8333333333333334,
        "Pima Indians Diabetes": 0.6948051948051948,
        "Australian": 0.9351145038167938,
        "Monks-1": 0.8214285714285714
      },
      "timestamp": 1767004941.86621,
      "total_score": 0.572790548492959
    },
    {
      "id": "04a6fe51-e229-4e9c-8ab9-19c9ea1f107e",
      "name": "GaussianNaiveBayes",
      "filename": "gaussian_naive_bayes.py",
      "description": "Synthesize a Gaussian Naive Bayes classifier. It should calculate the mean and standard deviation for each feature per class during training, and use the Gaussian probability density function to estimate likelihoods during prediction",
      "metrics": {
        "Iris": 0.9666666666666667,
        "Wine": 0.9722222222222222,
        "Breast Cancer": 0.9298245614035088,
        "Digits": 0.7416666666666667,
        "Balance Scale": 0.55,
        "Blood Transfusion": 0.76,
        "Haberman": 0.6612903225806451,
        "Seeds": 0.8809523809523809,
        "Teaching Assistant": 0.6129032258064516,
        "Zoo": 0.8571428571428571,
        "Planning Relax": 0.6216216216216216,
        "Ionosphere": 0.8873239436619719,
        "Sonar": 0.6428571428571429,
        "Glass": 0.5348837209302325,
        "Vehicle": 0.43529411764705883,
        "Liver Disorders": 0.2636986301369863,
        "Heart Statlog": 0.8518518518518519,
        "Pima Indians Diabetes": 0.7077922077922078,
        "Australian": 0.5114503816793893,
        "Monks-1": 0.6428571428571429
      },
      "timestamp": 1767004996.7146392,
      "total_score": 0.4322810157588755
    },
    {
      "id": "271796e7-90b4-459a-888c-a23295e9321f",
      "name": "LogisticRegressionClassifier",
      "filename": "logistic_regression_classifier.py",
      "description": "Create a Logistic Regression model from scratch. It must use a standard Sigmoid activation for binary tasks or Softmax for multi-class tasks. Implement a basic batch gradient descent optimizer with a fixed learning rate of 0.01 and a maximum of 100 iterations.",
      "metrics": {
        "Iris": 0.7666666666666667,
        "Wine": 0.9722222222222222,
        "Breast Cancer": 0.9473684210526315,
        "Digits": 0.8527777777777777,
        "Balance Scale": 0.65,
        "Blood Transfusion": 0.7733333333333333,
        "Haberman": 0.7419354838709677,
        "Seeds": 0.7857142857142857,
        "Teaching Assistant": 0.41935483870967744,
        "Zoo": 0.8095238095238095,
        "Planning Relax": 0.7027027027027027,
        "Ionosphere": 0.9014084507042254,
        "Sonar": 0.7619047619047619,
        "Glass": 0.5581395348837209,
        "Vehicle": 0.5294117647058824,
        "Liver Disorders": 0.25489236790606656,
        "Heart Statlog": 0.8148148148148148,
        "Pima Indians Diabetes": 0.7077922077922078,
        "Australian": 0.7213740458015268,
        "Monks-1": 0.6785714285714286
      },
      "timestamp": 1767005030.059626,
      "total_score": 0.40498803330325056
    },
    {
      "id": "73b8628b-f31f-4809-9c00-29d3ee3c1226",
      "name": "ManhattanWeightedKNNClassifier",
      "filename": "manhattan_weighted_knn_classifier.py",
      "description": "Synthesize a K-Nearest Neighbors classifier that uses the Manhattan distance metric instead of Euclidean. Implement a simple weighting mechanism where closer neighbors have a higher influence on the classification vote using an inverse-distance power of 2.",
      "metrics": {
        "Iris": 0.9666666666666667,
        "Wine": 1.0,
        "Breast Cancer": 0.9649122807017544,
        "Digits": 0.9888888888888889,
        "Balance Scale": 0.7,
        "Blood Transfusion": 0.7133333333333334,
        "Haberman": 0.5967741935483871,
        "Seeds": 0.8809523809523809,
        "Teaching Assistant": 0.6129032258064516,
        "Zoo": 0.9047619047619048,
        "Planning Relax": 0.6486486486486487,
        "Ionosphere": 0.9295774647887324,
        "Sonar": 0.7619047619047619,
        "Glass": 0.7674418604651163,
        "Vehicle": 0.711764705882353,
        "Liver Disorders": 0.9065557729941291,
        "Heart Statlog": 0.7407407407407407,
        "Pima Indians Diabetes": 0.7142857142857143,
        "Australian": 0.9045801526717557,
        "Monks-1": 0.8571428571428571
      },
      "timestamp": 1767005155.4203362,
      "total_score": 0.6888479213725884
    },
    {
      "id": "c9f0b184-cd54-4f16-86b8-72ba482a8d4e",
      "name": "TwoLayerNeuralNetClassifier",
      "filename": "two_layer_neural_net_classifier.py",
      "description": "create a fully connected 2 layer neural network",
      "metrics": {
        "Iris": 0.9666666666666667,
        "Wine": 1.0,
        "Breast Cancer": 0.9473684210526315,
        "Digits": 0.9805555555555555,
        "Balance Scale": 0.7,
        "Blood Transfusion": 0.8,
        "Haberman": 0.6451612903225806,
        "Seeds": 0.9047619047619048,
        "Teaching Assistant": 0.6129032258064516,
        "Zoo": 0.9047619047619048,
        "Planning Relax": 0.7297297297297297,
        "Ionosphere": 0.9577464788732394,
        "Sonar": 0.8809523809523809,
        "Glass": 0.6744186046511628,
        "Vehicle": 0.8470588235294118,
        "Liver Disorders": 0.6560665362035225,
        "Heart Statlog": 0.8148148148148148,
        "Pima Indians Diabetes": 0.7207792207792207,
        "Australian": 0.7557251908396947,
        "Monks-1": 1.0
      },
      "timestamp": 1767005209.0962,
      "total_score": 0.8087284676836257
    }
  ]
}