from sklearn.datasets import load_iris, load_wine, load_breast_cancer, load_digits, fetch_openml
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from data_loader import load_classification_datasets

import numpy as np
from sklearn.metrics import (
    accuracy_score,
)

from sklearn.base import BaseEstimator, clone
from typing import List, Tuple, Dict
import importlib.util
import inspect
import os


class BenchmarkSuite:
    def __init__(self, dataset_names=None, test_size: float = 0.2, random_state: int = 42, logging: bool = False, debugging: bool = False):
        self.test_size = test_size
        self.random_state = random_state
        self.dataset_names = dataset_names or ["Iris", "Wine", "Breast Cancer", "Digits"]
        self.datasets = load_classification_datasets(
            dataset_names=self.dataset_names,
            test_size=self.test_size,
            random_state=self.random_state,
            logging=logging,
        )
        self.logging = logging
        self.debugging = debugging
        self.results: Dict[str, Dict[str, Dict[str, float]]] = {}

    def run_benchmark(self, models: List[BaseEstimator]) -> Dict[str, Dict[str, float]]:
        for model in models:
            model_name = model.__class__.__name__
            self.results[model_name] = {}

            for dataset_name, (X_train, X_test, y_train, y_test) in self.datasets.items():
                try:
                    fresh_model = clone(model)
                    fresh_model.fit(X_train, y_train)
                    y_pred = fresh_model.predict(X_test)
                    score = accuracy_score(y_test, y_pred)
                    self.results[model_name][dataset_name] = {"Accuracy": score}
                except Exception as e:
                    msg = f"{model_name} failed on {dataset_name}: {type(e).__name__}: {e}"
                    if self.logging and self.debugging:
                        print(msg)
                    self.results[model_name][dataset_name] = {"error": msg}

                if self.logging: print(f"Evaluated {model_name} on {dataset_name}")

        return self.results

    def compute_aggregate_relative_score_strict(self):
        datasets = list(self.datasets.keys())
        models = list(self.results.keys())

        raw_by_dataset = {d: {} for d in datasets}
        for d in datasets:
            for m in models:
                cell = self.results.get(m, {}).get(d, {})
                if "Accuracy" in cell:
                    raw_by_dataset[d][m] = float(cell["Accuracy"])

        norm_by_dataset = {d: {} for d in datasets}
        for d in datasets:
            vals = list(raw_by_dataset[d].values())
            if not vals:
                continue
            mn, mx = min(vals), max(vals)
            denom = mx - mn
            for m, s in raw_by_dataset[d].items():
                norm_by_dataset[d][m] = 1.0 if denom == 0 else (s - mn) / denom

        aggregate = {}
        for m in models:
            total = 0.0
            for d in datasets:
                total += norm_by_dataset[d].get(m, 0.0)
            aggregate[m] = total / len(datasets) if datasets else 0.0

        return aggregate, norm_by_dataset

    def print_table(self):
        datasets = list(self.datasets.keys())
        models = list(self.results.keys())

        aggregate, _ = self.compute_aggregate_relative_score_strict()

        colw = max(12, max(len(d) for d in datasets) + 2)
        roww = max(24, max(len(m) for m in models) + 2)

        header = (
            "Model".ljust(roww)
            + "".join(d.ljust(colw) for d in datasets)
            + "Aggregate".ljust(colw)
        )
        print(header)
        print("-" * len(header))
        models = sorted(models, key=lambda m: aggregate.get(m, 0.0), reverse=True)

        for model_name in models:
            row = model_name.ljust(roww)
            for dataset_name in datasets:
                cell = self.results.get(model_name, {}).get(dataset_name, {})
                if "Accuracy" in cell:
                    row += f"{cell['Accuracy']:.4f}".ljust(colw)
                else:
                    row += "ERR".ljust(colw)
            row += f"{aggregate.get(model_name, 0.0):.3f}".ljust(colw)
            print(row)

    def save_latex_table_multirow(self, filepath: str, caption: str = "Benchmark results", label: str = "tab:benchmark"):
        datasets = list(self.datasets.keys())
        models = list(self.results.keys())

        aggregate, _ = self.compute_aggregate_relative_score_strict()
        models = sorted(models, key=lambda mn: aggregate.get(mn, 0.0), reverse=True)

        os.makedirs(os.path.dirname(filepath) or ".", exist_ok=True)

        with open(filepath, "w") as f:
            f.write("% Auto-generated by BenchmarkSuite.save_latex_table_multirow (classification-only)\n")
            f.write("% Requires LaTeX packages: booktabs, multirow\n\n")

            f.write("\\begin{table}[ht]\n")
            f.write("\\centering\n")
            f.write(f"\\caption{{{caption}}}\n")
            f.write(f"\\label{{{label}}}\n")

            f.write("\\begin{tabular}{l" + "c" * (len(datasets) + 1) + "}\n")
            f.write("\\toprule\n")

            f.write(
                "Model"
                + f" & \\multicolumn{{{len(datasets)}}}{{c}}{{Classification}}"
                + " & \\multicolumn{1}{c}{RelAgg}"
                + " \\\\\n"
            )
            f.write(f"\\cmidrule(lr){{2-{1 + len(datasets)}}}\n")

            header2 = " "
            for d in datasets:
                header2 += f" & \\multicolumn{{1}}{{c}}{{{d}}}"
            header2 += " & "
            f.write(header2 + " \\\\\n")

            header3 = " "
            for _ in datasets:
                header3 += " & Accuracy"
            header3 += " & Rel"
            f.write(header3 + " \\\\\n")

            f.write("\\midrule\n")

            for model_name in models:
                row = model_name
                for d in datasets:
                    cell = self.results.get(model_name, {}).get(d, {})
                    if "Accuracy" in cell:
                        row += f" & {cell['Accuracy']:.4f}"
                    else:
                        row += " & --"
                row += f" & {aggregate.get(model_name, 0.0):.3f}"
                f.write(row + " \\\\\n")

            f.write("\\bottomrule\n")
            f.write("\\end{tabular}\n")
            f.write("\\end{table}\n")

        print(f"Saved LaTeX table to: {filepath}")
        print("!!!Make sure to include \\\\usepackage{booktabs} and \\\\usepackage{multirow}")


def load_models_from_directory(models_dir: str, logging: bool = False):
    models = []

    for filename in sorted(os.listdir(models_dir)):
        if not filename.endswith(".py"):
            continue
        if filename == "__init__.py":
            continue

        file_path = os.path.join(models_dir, filename)
        module_name = os.path.splitext(filename)[0]

        spec = importlib.util.spec_from_file_location(module_name, file_path)
        if spec is None or spec.loader is None:
            continue

        module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(module)

        for name, obj in inspect.getmembers(module, inspect.isclass):
            if obj.__module__ != module.__name__:
                continue
            if not hasattr(obj, "fit") or not hasattr(obj, "predict"):
                continue

            try:
                instance = obj()
                models.append(instance)
                if logging:
                    print(f"Loaded model: {obj.__name__} from {filename}")
            except Exception as e:
                if logging:
                    print(f"Skipped {obj.__name__} (could not instantiate): {e}")

    return models


def main():
    logging = True
    
    current_dir = os.path.dirname(os.path.abspath(__file__))
    metaomni_dir = os.path.join(current_dir, "metaomni")
    models_dir = [metaomni_dir]

    # SUPPORTED_DATASETS = ["Iris", "Wine", "Breast Cancer", "Digits", "Adult", "Bank Marketing", "Credit-G", "Phoneme", "Spambase", "Ionosphere", "Sonar", "Vehicle", "Glass"]
    suite = BenchmarkSuite(dataset_names=[
        "Iris",
        "Breast Cancer",
        "Adult",
        "Credit-G",
        "Vehicle",
        "Glass",
        ],
        logging=logging,
        debugging=False,
    )

    models = []
    for dir in models_dir:
        print(f"loading {len(os.listdir(dir))} files from {dir}") 
        models.extend(load_models_from_directory(dir, logging))

    if not models:
        print("No valid models found.")
        return

    suite.run_benchmark(models[:5])
    suite.print_table()

    suite.save_latex_table_multirow(
        filepath=f"{current_dir}/results/benchmark_multirow_25_12_18.tex",
        caption="MetaOmni-generated models evaluated on classification datasets.",
        label="tab:metaomni-classification-benchmark",
    )

if __name__ == "__main__":
    main()
